# Guia de Uso - Web Scraper AI

## IntroduÃ§Ã£o

Este guia completo demonstra como usar o Web Scraper AI, desde instalaÃ§Ã£o atÃ© casos de uso avanÃ§ados.

## InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Requisitos do Sistema
```bash
# Python 3.8+
python --version

# Chrome/Chromium browser
google-chrome --version
```

### 2. InstalaÃ§Ã£o de DependÃªncias
```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Verificar instalaÃ§Ã£o
python -c "import selenium, openai, pandas; print('Dependencies OK')"
```

### 3. ConfiguraÃ§Ã£o de Ambiente
```bash
# Copiar arquivo de exemplo
cp .env.example .env

# Editar configuraÃ§Ãµes
nano .env
```

**ConfiguraÃ§Ãµes obrigatÃ³rias no .env:**
```env
# OpenAI API (para recursos de IA)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Chrome Driver (deixe vazio para auto-download)
CHROME_DRIVER_PATH=

# ConfiguraÃ§Ãµes de logging
LOG_LEVEL=INFO
LOG_FILE=logs/application.log

# ConfiguraÃ§Ãµes de performance
MAX_WORKERS=3
DEFAULT_TIMEOUT=30
```

## Uso BÃ¡sico

### 1. Executando a AplicaÃ§Ã£o

#### Modo Interativo (Recomendado para iniciantes)
```bash
# Iniciar no modo interativo
python main.py

# Com configuraÃ§Ãµes especÃ­ficas
python main.py --mode interactive --workers 5 --log-level DEBUG
```

#### Modo Daemon (Para execuÃ§Ã£o contÃ­nua)
```bash
# Executar em background
python main.py --mode daemon

# Com configuraÃ§Ãµes
python main.py --mode daemon --workers 10 --log-level INFO
```

### 2. Interface Interativa

Quando executado no modo interativo, vocÃª verÃ¡ um menu como este:

```
ğŸ¤– Web Scraper AI - Interactive Mode
==================================================
Welcome to the interactive command interface!
Type 'help' or '?' for available commands.
==================================================

ğŸ“‹ Available Commands:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. status     - Show application status
2. health     - Show health check
3. metrics    - Show performance metrics
4. create-task - Create a scraping task
5. list-tasks - List all tasks
6. cleanup    - Run maintenance cleanup
7. backup     - Create system backup
8. exit       - Exit application
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ Enter command:
```

### 3. Comandos Principais

#### Status da AplicaÃ§Ã£o
```bash
# Comando: status ou 1
ğŸ“Š Application Status:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”§ Initialized: âœ… Yes
â–¶ï¸  Running: âœ… Yes
ğŸŒ Site Configs: 2
âš™ï¸  Engine Status: RUNNING
ğŸ”„ Active Tasks: 1
ğŸ“¦ Queue Size: 3
âœ… Completed Tasks: 15
```

#### VerificaÃ§Ã£o de SaÃºde
```bash
# Comando: health ou 2
ğŸ¥ Health Check Results:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Overall Status: HEALTHY
ğŸ• Checked at: 2024-01-15 14:30:00

ğŸ”§ Component Health:
  âœ… File Manager: operational
  âœ… Ai Client: connected
  âœ… Navigator: ready
  âœ… Pdf Processor: available
```

#### MÃ©tricas de Performance
```bash
# Comando: metrics ou 3
ğŸ“ˆ Performance Metrics:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Tasks Completed: 45
âŒ Tasks Failed: 2
ğŸ“Š Success Rate: 95.74%
â±ï¸  Average Time: 12.50s
ğŸ• Uptime: 3600s

ğŸ’¾ Storage:
ğŸ“ Total Files: 156
ğŸ’¿ Total Size: 45.67 MB
```

## ConfiguraÃ§Ã£o de Sites

### 1. Estrutura de ConfiguraÃ§Ã£o

Crie um arquivo JSON na pasta `config/sites/` para cada site:

```json
{
  "name": "example_news_site",
  "base_url": "https://example-news.com",
  "description": "Example news website scraping",
  "extraction_rules": [
    {
      "name": "title",
      "selector": {
        "type": "css",
        "value": "h1.article-title",
        "wait_condition": "presence"
      },
      "data_type": "text",
      "required": true
    },
    {
      "name": "content",
      "selector": {
        "type": "css",
        "value": ".article-content p",
        "wait_condition": "presence"
      },
      "data_type": "text",
      "required": true
    },
    {
      "name": "author",
      "selector": {
        "type": "css",
        "value": ".author-name",
        "wait_condition": "presence"
      },
      "data_type": "text",
      "required": false,
      "default_value": "Unknown Author"
    },
    {
      "name": "publish_date",
      "selector": {
        "type": "css",
        "value": "time[datetime]",
        "wait_condition": "presence",
        "attribute": "datetime"
      },
      "data_type": "datetime",
      "required": false
    }
  ],
  "navigation": {
    "start_urls": [
      "https://example-news.com/latest",
      "https://example-news.com/tech"
    ],
    "pagination": {
      "next_button_selector": ".pagination .next",
      "max_pages": 10
    }
  },
  "scraping_config": {
    "wait_time": 3.0,
    "timeout": 30.0,
    "retry_attempts": 3,
    "use_ai_navigation": true,
    "respect_robots_txt": true
  }
}
```

### 2. Tipos de Seletores

#### CSS Selectors
```json
{
  "name": "title",
  "selector": {
    "type": "css",
    "value": "h1.main-title",
    "wait_condition": "clickable"
  }
}
```

#### XPath Selectors
```json
{
  "name": "price",
  "selector": {
    "type": "xpath",
    "value": "//span[@class='price'][1]/text()",
    "wait_condition": "presence"
  }
}
```

#### AI-Powered Selectors
```json
{
  "name": "product_description",
  "selector": {
    "type": "ai",
    "description": "Find the main product description text",
    "fallback_selector": ".description, .product-info p"
  }
}
```

## Casos de Uso AvanÃ§ados

### 1. E-commerce Scraping

#### ConfiguraÃ§Ã£o para Site de E-commerce
```json
{
  "name": "ecommerce_products",
  "base_url": "https://shop.example.com",
  "extraction_rules": [
    {
      "name": "product_name",
      "selector": {"type": "css", "value": "h1.product-title"},
      "data_type": "text",
      "required": true
    },
    {
      "name": "price",
      "selector": {"type": "css", "value": ".price-current"},
      "data_type": "decimal",
      "required": true,
      "transform": "extract_numbers"
    },
    {
      "name": "availability",
      "selector": {"type": "css", "value": ".stock-status"},
      "data_type": "text",
      "required": false
    },
    {
      "name": "images",
      "selector": {"type": "css", "value": ".product-images img"},
      "data_type": "list",
      "attribute": "src",
      "required": false
    },
    {
      "name": "specifications",
      "selector": {"type": "ai", "description": "Extract product specifications table"},
      "data_type": "structured",
      "required": false
    }
  ],
  "navigation": {
    "start_urls": ["https://shop.example.com/products"],
    "follow_links": {
      "selector": ".product-link",
      "max_depth": 2
    }
  }
}
```

#### Script de AutomaÃ§Ã£o
```python
#!/usr/bin/env python3
"""
Script para scraping automatizado de e-commerce
"""

from src.core.application import app
from src.models.scraping_task import TaskPriority
import time

def scrape_ecommerce_daily():
    """Executa scraping diÃ¡rio de produtos"""
    
    # Initialize application
    app.initialize()
    app.start()
    
    try:
        # Create scraping task
        task = app.create_scraping_task(
            site_name='ecommerce_products',
            priority=TaskPriority.HIGH
        )
        
        print(f"âœ… Task created: {task.task_id}")
        
        # Monitor progress
        while True:
            status = app.get_application_status()
            active_tasks = status.get('engine_status', {}).get('active_tasks', 0)
            
            if active_tasks == 0:
                print("âœ… Scraping completed!")
                break
                
            print(f"ğŸ”„ Active tasks: {active_tasks}")
            time.sleep(30)
        
        # Get metrics
        metrics = app.get_metrics()
        print(f"ğŸ“Š Success rate: {metrics['engine']['success_rate']:.2%}")
        
    finally:
        app.stop()

if __name__ == "__main__":
    scrape_ecommerce_daily()
```

### 2. News Monitoring

#### ConfiguraÃ§Ã£o para Monitoramento de NotÃ­cias
```json
{
  "name": "news_monitor",
  "base_url": "https://news.example.com",
  "extraction_rules": [
    {
      "name": "headline",
      "selector": {"type": "css", "value": "h2.headline"},
      "data_type": "text",
      "required": true
    },
    {
      "name": "summary",
      "selector": {"type": "ai", "description": "Extract article summary or first paragraph"},
      "data_type": "text",
      "required": true
    },
    {
      "name": "sentiment",
      "selector": {"type": "ai", "description": "Analyze article sentiment"},
      "data_type": "structured",
      "ai_analysis": "sentiment"
    },
    {
      "name": "keywords",
      "selector": {"type": "ai", "description": "Extract key topics and entities"},
      "data_type": "list",
      "ai_analysis": "keywords"
    }
  ],
  "scheduling": {
    "frequency": "hourly",
    "max_articles_per_run": 50
  }
}
```

### 3. Document Processing

#### Processamento de PDFs
```python
from src.ai.pdf_processor_agent import PDFProcessorAgent
from src.ai.openai_client import OpenAIClient

# Initialize PDF processor
client = OpenAIClient()
pdf_processor = PDFProcessorAgent(client)

# Process financial report
result = pdf_processor.process_pdf_file(
    "quarterly_report.pdf", 
    analysis_type="financial"
)

# Extract key financial metrics
financial_data = pdf_processor.extract_structured_data(
    "quarterly_report.pdf",
    target_fields=[
        "total_revenue", 
        "net_profit", 
        "cash_flow",
        "total_assets",
        "debt_ratio"
    ]
)

print("ğŸ“Š Financial Metrics:")
for field, value in financial_data['extracted_data'].items():
    confidence = financial_data['confidence_scores'][field]
    print(f"  {field}: {value} (confidence: {confidence:.2%})")
```

## Monitoramento e Maintenance

### 1. Logs e Debugging

#### Estrutura de Logs
```
logs/
â”œâ”€â”€ application.log          # Log principal
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ 2024-01-15/         # Logs por data
â”‚   â”‚   â”œâ”€â”€ tasks.log       # Log de tarefas
â”‚   â”‚   â””â”€â”€ errors.log      # Log de erros
â”‚   â””â”€â”€ archive/            # Logs arquivados
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ openai_usage.log    # Uso da API OpenAI
â”‚   â””â”€â”€ ai_analysis.log     # AnÃ¡lises de IA
â””â”€â”€ performance/
    â”œâ”€â”€ metrics.log         # MÃ©tricas de performance
    â””â”€â”€ system.log          # MÃ©tricas do sistema
```

#### ConfiguraÃ§Ã£o de Log Level
```bash
# Debug (muito detalhado)
python main.py --log-level DEBUG

# Info (padrÃ£o)
python main.py --log-level INFO

# Warning (apenas avisos e erros)
python main.py --log-level WARNING

# Error (apenas erros)
python main.py --log-level ERROR
```

### 2. Backup e Recovery

#### Backup Manual
```bash
# Comando: backup ou 7
ğŸ’¾ Creating System Backup...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Backup Created Successfully!
ğŸ†” Backup ID: backup_2024-01-15_14-30-00
ğŸ“ Files: 1,245
ğŸ’¿ Size: 156.78 MB
ğŸ“… Created: 2024-01-15 14:30:00
```

#### Backup AutomÃ¡tico
```python
import schedule
import time
from src.core.application import app

def automated_backup():
    """Backup automÃ¡tico diÃ¡rio"""
    try:
        backup_info = app.create_backup("Automated daily backup")
        print(f"âœ… Backup created: {backup_info['backup_id']}")
        
        # Cleanup old backups (manter apenas 7)
        cleanup_result = app.file_manager.cleanup_old_backups(keep_count=7)
        print(f"ğŸ§¹ Cleaned up {cleanup_result['removed_backups']} old backups")
        
    except Exception as e:
        print(f"âŒ Backup failed: {e}")

# Agendar backup diÃ¡rio Ã s 2:00 AM
schedule.every().day.at("02:00").do(automated_backup)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### 3. Cleanup e ManutenÃ§Ã£o

#### Limpeza Manual
```bash
# Comando: cleanup ou 6
ğŸ§¹ Running Maintenance Cleanup...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Cleanup Completed Successfully!
ğŸ—‘ï¸  Temp Files: 45 files deleted
ğŸ’¾ Space Freed: 23.45 MB
ğŸ“¦ Archived: 12 files
ğŸ—‚ï¸  Old Tasks: 156 removed from memory
```

#### Limpeza AutomÃ¡tica
```python
from src.core.application import app
import threading
import time

def periodic_cleanup():
    """Limpeza automÃ¡tica a cada 6 horas"""
    while True:
        try:
            # Wait 6 hours
            time.sleep(6 * 3600)
            
            # Run cleanup
            cleanup_result = app.cleanup_old_data()
            print(f"ğŸ§¹ Auto cleanup: {cleanup_result}")
            
        except Exception as e:
            print(f"âŒ Auto cleanup failed: {e}")

# Start cleanup thread
cleanup_thread = threading.Thread(target=periodic_cleanup, daemon=True)
cleanup_thread.start()
```

## Troubleshooting

### 1. Problemas Comuns

#### WebDriver Issues
```bash
# Error: ChromeDriver not found
# Solution: Install or update Chrome
sudo apt-get update
sudo apt-get install google-chrome-stable

# Or download ChromeDriver manually
wget https://chromedriver.storage.googleapis.com/LATEST_RELEASE
```

#### OpenAI API Issues
```python
# Test OpenAI connection
from src.ai.openai_client import OpenAIClient

try:
    client = OpenAIClient()
    response = client.generate_completion("Test connection", max_tokens=10)
    print("âœ… OpenAI connection OK")
except Exception as e:
    print(f"âŒ OpenAI error: {e}")
```

#### Memory Issues
```bash
# Monitor memory usage
python -c "
import psutil
print(f'Memory: {psutil.virtual_memory().percent}%')
print(f'CPU: {psutil.cpu_percent()}%')
"

# Reduce workers if needed
python main.py --workers 1
```

### 2. Performance Optimization

#### ConfiguraÃ§Ãµes Recomendadas

**Para mÃ¡quina de desenvolvimento:**
```env
MAX_WORKERS=2
DEFAULT_TIMEOUT=30
LOG_LEVEL=INFO
```

**Para servidor de produÃ§Ã£o:**
```env
MAX_WORKERS=8
DEFAULT_TIMEOUT=60
LOG_LEVEL=WARNING
```

**Para recursos limitados:**
```env
MAX_WORKERS=1
DEFAULT_TIMEOUT=45
LOG_LEVEL=ERROR
```

## IntegraÃ§Ãµes

### 1. IntegraÃ§Ã£o com Databases

```python
import sqlite3
from src.core.application import app

def save_to_database(scraping_result):
    """Salva resultados no banco de dados"""
    
    conn = sqlite3.connect('scraping_data.db')
    cursor = conn.cursor()
    
    # Create table if not exists
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS scraped_data (
            id INTEGER PRIMARY KEY,
            site_name TEXT,
            url TEXT,
            title TEXT,
            content TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Insert data
    cursor.execute('''
        INSERT INTO scraped_data (site_name, url, title, content)
        VALUES (?, ?, ?, ?)
    ''', (
        scraping_result['site_name'],
        scraping_result['url'],
        scraping_result['data'].get('title'),
        scraping_result['data'].get('content')
    ))
    
    conn.commit()
    conn.close()
```

### 2. IntegraÃ§Ã£o com APIs

```python
import requests
from src.core.application import app

def send_to_webhook(data):
    """Envia dados para webhook externo"""
    
    webhook_url = "https://your-api.com/webhook"
    
    payload = {
        'source': 'web_scraper_ai',
        'timestamp': data['scraped_at'],
        'data': data['extracted_data']
    }
    
    try:
        response = requests.post(webhook_url, json=payload)
        response.raise_for_status()
        print(f"âœ… Data sent to webhook: {response.status_code}")
    except Exception as e:
        print(f"âŒ Webhook failed: {e}")
```

---

**Este guia fornece uma base sÃ³lida para usar o Web Scraper AI eficientemente. Para casos de uso especÃ­ficos, consulte a documentaÃ§Ã£o tÃ©cnica detalhada de cada componente.**